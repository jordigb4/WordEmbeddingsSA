{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:Courier New; color:#CCCCCC\">**Text Similarity**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Load Data and Imports**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%pip install datasets\\n%pip install \\n%pip install -U spacy\\n!python3 -m spacy download ca_core_news_md\\n!python3 -m spacy download ca_core_news_trf\\n%pip install spacy-transformers\\n%pip install scipy\\n%pip install tensorflow\\n%pip install transformers\\n%pip install pandas\\n'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%pip install datasets\n",
    "%pip install \n",
    "%pip install -U spacy\n",
    "!python3 -m spacy download ca_core_news_md\n",
    "!python3 -m spacy download ca_core_news_trf\n",
    "%pip install spacy-transformers\n",
    "%pip install scipy\n",
    "%pip install tensorflow\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requisites\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from gensim.models import KeyedVectors,TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from typing import Tuple, List\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from gensim.matutils import corpus2csc\n",
    "import scipy\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jordi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for projecte-aina/sts-ca contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/projecte-aina/sts-ca\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "dataset = load_dataset(\"projecte-aina/sts-ca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Preprocessing and dataframes creation**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to preprocess the data in incontextual embedding models, we will stablish stopword treatment and simple_preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_CAT ={\n",
    "    \"a\", \"abans\", \"algun\", \"alguna\", \"algunes\", \"alguns\", \"altre\", \"amb\", \"ambdós\", \"anar\", \n",
    "    \"ans\", \"aquell\", \"aquelles\", \"aquells\", \"aquí\", \"bastant\", \"bé\", \"cada\", \"com\", \n",
    "    \"consegueixo\", \"conseguim\", \"conseguir\", \"consigueix\", \"consigueixen\", \"consigueixes\", \n",
    "    \"dalt\", \"de\", \"des\", \"dins\", \"el\", \"elles\", \"ells\", \"els\", \"en\", \"ens\", \"entre\", \n",
    "    \"era\", \"erem\", \"eren\", \"eres\", \"es\", \"és\", \"éssent\", \"està\", \"estan\", \"estat\", \n",
    "    \"estava\", \"estem\", \"esteu\", \"estic\", \"ets\", \"fa\", \"faig\", \"fan\", \"fas\", \"fem\", \n",
    "    \"fer\", \"feu\", \"fi\", \"haver\", \"i\", \"inclòs\", \"jo\", \"la\", \"les\", \"llarg\", \"llavors\", \n",
    "    \"mentre\", \"meu\", \"mode\", \"molt\", \"molts\", \"nosaltres\", \"o\", \"on\", \"per\", \"però\", \n",
    "    \"perquè\", \"podem\", \"poden\", \"poder\", \"podeu\", \"potser\", \"primer\", \"puc\", \"quan\", \n",
    "    \"quant\", \"que\", \"què\", \"qui\", \"sabem\", \"saben\", \"saber\", \"sabeu\", \"sap\", \"saps\", \n",
    "    \"sense\", \"ser\", \"seu\", \"seus\", \"si\", \"soc\", \"solament\", \"sols\", \"som\", \"sota\", \n",
    "    \"també\", \"te\", \"tene\", \"tenim\", \"tenir\", \"teniu\", \"teu\", \"tinc\", \"tot\", \"últim\", \n",
    "    \"un\", \"una\", \"unes\", \"uns\", \"ús\", \"va\", \"vaig\", \"van\", \"vosaltres\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing\n",
    "def preprocess(sentence: str, stop:bool = True) -> List[str]:\n",
    "    preprocessed = simple_preprocess(sentence)\n",
    "    if stop: preprocessed = [token for token in preprocessed if token not in STOPWORDS_CAT]\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:Courier New; color:#336633\">**Dataset format creation**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**Count-Vectorizer/TF-IDF**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pairs = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in dataset[\"train\"].to_list()]\n",
    "input_pairs_val = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in dataset[\"validation\"].to_list()]\n",
    "input_pairs_test = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in dataset[\"test\"].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_input_pairs = input_pairs + input_pairs_val + input_pairs_test\n",
    "# Preprocessament de les frases i creació dels diccionaris\n",
    "\n",
    "# Frases per a models contextuals, amb stopwords\n",
    "sentences_1 = [preprocess(sentence_1,stop = False) for sentence_1, _, _ in all_input_pairs]\n",
    "sentences_2 = [preprocess(sentence_2,stop = False) for _, sentence_2, _ in all_input_pairs]\n",
    "\n",
    "# Frases per a models no contextuals, sense stopwords\n",
    "sentences_1_preproc = [preprocess(sentence_1) for sentence_1, _, _ in all_input_pairs]\n",
    "sentences_2_preproc = [preprocess(sentence_2) for _, sentence_2, _ in all_input_pairs]\n",
    "\n",
    "sentence_pairs_preproc = list(zip(sentences_1_preproc, sentences_2_preproc))\n",
    "sentence_pairs = list(zip(sentences_1, sentences_2))\n",
    "\n",
    "# Versió aplanada de les frases\n",
    "sentences_pairs_flattened_preproc = sentences_1_preproc + sentences_2_preproc\n",
    "sentences_pairs_flattened = sentences_1 + sentences_2\n",
    "\n",
    "dict_preproc = Dictionary(sentences_pairs_flattened_preproc)\n",
    "dict_preproc_complete = Dictionary(sentences_pairs_flattened)\n",
    "\n",
    "# Filtrem tamany de diccionari per les variants estàndard de TF-IDF i BOW\n",
    "\n",
    "dict_preproc.filter_extremes(keep_n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de pesos TF-IDF per les frases preprocessades\n",
    "corpus = [dict_preproc.doc2bow(sent) for sent in sentences_pairs_flattened_preproc]\n",
    "corpus_complete = [dict_preproc_complete.doc2bow(sent) for sent in sentences_pairs_flattened_preproc]\n",
    "model_tfidf = TfidfModel(corpus)\n",
    "model_tfidf_complete = TfidfModel(corpus_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get count vector from dictionary\n",
    "def map_to_bow(sentence: List[str], dictionary: Dictionary) -> np.ndarray:\n",
    "    vec = np.zeros(len(dictionary))   \n",
    "    bow = dictionary.doc2bow(sentence)\n",
    "    for token_id, count in bow:\n",
    "        vec[token_id] = count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_tf_idf(sentence: List[str], dictionary: Dictionary, tfidf: TfidfModel) -> np.ndarray:\n",
    "    vec = np.zeros(len(dictionary))   \n",
    "    bow = dictionary.doc2bow(sentence)   \n",
    "    for token_id, value in tfidf[bow]:\n",
    "        vec[token_id] = value\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process all pairs\n",
    "def bow_pairs(sentence_pairs: List[Tuple[str, str, float]], dictionary: Dictionary = None,tf:bool = False,model_tfidf:TfidfModel = None) -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "\n",
    "    pair_vectors = []\n",
    "    for (s1,s2,sim) in sentence_pairs:\n",
    "        \n",
    "        s1_preproc = preprocess(s1)\n",
    "        s2_preproc = preprocess(s2)\n",
    "        \n",
    "        if tf == False:\n",
    "            vectors_1 = map_to_bow(s1_preproc, dictionary)\n",
    "            vectors_2 = map_to_bow(s2_preproc, dictionary)\n",
    "        else:\n",
    "            vectors_1 = map_to_tf_idf(s1_preproc, dictionary, model_tfidf)\n",
    "            vectors_2 = map_to_tf_idf(s2_preproc, dictionary, model_tfidf)\n",
    "\n",
    "        pair_vectors.append(((vectors_1, vectors_2), sim))\n",
    "    return pair_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW pairs\n",
    "df_bow_train = bow_pairs(input_pairs, dict_preproc)\n",
    "df_bow_val = bow_pairs(input_pairs_val, dict_preproc)\n",
    "df_bow_test = bow_pairs(input_pairs_test, dict_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF pairs\n",
    "df_tfidf_train = bow_pairs(input_pairs, dict_preproc,tf = True,model_tfidf = model_tfidf)\n",
    "df_tfidf_val = bow_pairs(input_pairs_val, dict_preproc,tf = True,model_tfidf = model_tfidf)\n",
    "df_tfidf_test = bow_pairs(input_pairs_test, dict_preproc,tf = True,model_tfidf = model_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We erased stopwords in TF-IDF and BOW models. We therefore expect that the differences between TF-IDF and BOW are not that notable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**Word2Vec/GloVe**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"font-family:Courier New; color:#994C00\">**Load Vectors**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will used pretrained catalan Word2Vec Continous Skipgram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_FILE = 'C:/Users/Jordi/Desktop/Universitat/PLH/Models/cat_w2vec/model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the words and their corresponding vectors\n",
    "wv_model = KeyedVectors.load_word2vec_format(WORD_EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tf_idf(sentence_preproc: List[str], dictionary: Dictionary, tf_idf_model: TfidfModel) -> Tuple[List[np.ndarray], List[float]]:\n",
    "    bow = dictionary.doc2bow(sentence_preproc)\n",
    "    tf_idf = tf_idf_model[bow]\n",
    "    vectors, weights = [], []\n",
    "    for word_index, weight in tf_idf:\n",
    "        word = dictionary.get(word_index)\n",
    "        if word in wv_model:\n",
    "            vectors.append(wv_model[word])\n",
    "            weights.append(weight)\n",
    "    return vectors, weights\n",
    "\n",
    "def map_pairs(\n",
    "        sentence_pairs: List[Tuple[str, str, float]],\n",
    "        dictionary: Dictionary = None,\n",
    "        tf_idf_model: TfidfModel = None,\n",
    ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "    # Mapeo de los pares de oraciones a pares de vectores\n",
    "    pares_vectores = []\n",
    "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
    "        sentence_1_preproc = preprocess(sentence_1)\n",
    "        sentence_2_preproc = preprocess(sentence_2)\n",
    "        # Si usamos TF-IDF\n",
    "        if tf_idf_model is not None:\n",
    "            # Cálculo del promedio ponderado por TF-IDF de los word embeddings\n",
    "            vectors1, weights1 = map_tf_idf(sentence_1_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, )\n",
    "            vectors2, weights2 = map_tf_idf(sentence_2_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, )\n",
    "            vector1 = np.average(vectors1, weights=weights1, axis=0, )\n",
    "            vector2 = np.average(vectors2, weights=weights2, axis=0, )\n",
    "        else:\n",
    "            # Cálculo del promedio de los word embeddings\n",
    "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
    "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
    "            vector1 = np.mean(vectors1, axis=0)\n",
    "            vector2 = np.mean(vectors2, axis=0)\n",
    "        # Añadir a la lista\n",
    "        pares_vectores.append(((vector1, vector2), similitud))\n",
    "    return pares_vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF + W2VEC pairs\n",
    "df_w2vec_tf_train = map_pairs(input_pairs,  tf_idf_model=model_tfidf_complete, dictionary= dict_preproc_complete, )\n",
    "df_w2vec_tf_val = map_pairs(input_pairs_val, tf_idf_model=model_tfidf_complete, dictionary= dict_preproc_complete, )\n",
    "df_w2vec_tf_test = map_pairs(input_pairs_test, tf_idf_model=model_tfidf_complete, dictionary= dict_preproc_complete, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pairs\n",
    "df_w2vec_train = map_pairs(sentence_pairs = input_pairs,dictionary= dict_preproc_complete)\n",
    "df_w2vec_val = map_pairs(sentence_pairs = input_pairs_val,dictionary= dict_preproc_complete)\n",
    "df_w2vec_test = map_pairs(sentence_pairs = input_pairs_test,dictionary= dict_preproc_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**spaCy**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ca_core_news_md') # Load catalan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_spacy(sentence_pairs: List[Tuple[str, str, float]], nlp: spacy.language.Language) -> np.ndarray:\n",
    "\n",
    "    pares_vectores = []\n",
    "    #Per cada frase\n",
    "    for s1,s2,sim in sentence_pairs:\n",
    "\n",
    "        vector1 = nlp(s1).vector\n",
    "        vector2 = nlp(s2).vector\n",
    "\n",
    "        #Afegim vector a llista\n",
    "        pares_vectores.append(((vector1, vector2), sim))\n",
    "        \n",
    "    return pares_vectores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY DATAFRAMES\n",
    "df_spacy_train = map_to_spacy(input_pairs, nlp)\n",
    "df_spacy_val = map_to_spacy(input_pairs_val, nlp)\n",
    "df_spacy_test = map_to_spacy(input_pairs_test, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**RoBERTa CLS/Mitjana**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_r = spacy.load('ca_core_news_trf') # Catalan transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_transformer(sentence_pairs: List[Tuple[str, str, float]], nlp: spacy.language.Language,cls:str = True) -> np.ndarray:\n",
    "\n",
    "    pares_vectores = []\n",
    "    #Per cada frase\n",
    "    for s1,s2,sim in sentence_pairs:\n",
    "\n",
    "        #Si volem el vector CLS\n",
    "        \n",
    "        if cls:\n",
    "            vector1 = nlp(s1)._.trf_data.last_hidden_layer_state.data[0]\n",
    "            vector2 = nlp(s2)._.trf_data.last_hidden_layer_state.data[0]\n",
    "\n",
    "        #Si volem la mitjana dels valors de les frases\n",
    "        else:\n",
    "\n",
    "            vectors1 = nlp(s1)._.trf_data.last_hidden_layer_state.data[1:]\n",
    "            vectors2 = nlp(s2)._.trf_data.last_hidden_layer_state.data[1:]\n",
    "\n",
    "            vector1 = np.mean(vectors1, axis=0)\n",
    "            vector2 = np.mean(vectors2, axis=0)\n",
    "\n",
    "        #Afegim vector a llista\n",
    "        pares_vectores.append(((vector1, vector2), sim))\n",
    "        \n",
    "    return pares_vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLS BERT\n",
    "df_BERT_CLS = map_transformer(input_pairs, nlp_r)\n",
    "df_BERT_CLS_val = map_transformer(input_pairs_val, nlp_r)\n",
    "df_BERT_CLS_test = map_transformer(input_pairs_test, nlp_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN BERT\n",
    "df_BERT_MEAN = map_transformer(input_pairs, nlp_r,cls = False)\n",
    "df_BERT_MEAN_val = map_transformer(input_pairs_val, nlp_r,cls = False)\n",
    "df_BERT_MEAN_test = map_transformer(input_pairs_test, nlp_r,cls = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As Roberta cased Finetuned returns the probability, we will be showing the results at last, just after the model embedding representation comparison.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Model creation**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the example delievered to us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model_better(embedding_size: int = 300, learning_rate: float = 1e-3) -> tf.keras.Model:\n",
    "    # Capa de entrada para los pares de vectores\n",
    "    input_1 = tf.keras.Input(shape=(embedding_size,))\n",
    "    input_2 = tf.keras.Input(shape=(embedding_size,))\n",
    "\n",
    "    # Hidden layer\n",
    "    first_projection = tf.keras.layers.Dense(\n",
    "        embedding_size,\n",
    "        kernel_initializer=tf.keras.initializers.Identity(),\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "    )\n",
    "    projected_1 =  first_projection(input_1)\n",
    "    projected_2 = first_projection(input_2)\n",
    "    \n",
    "    # Compute the cosine distance using a Lambda layer\n",
    "    def normalized_product(x):\n",
    "        x1, x2 = x\n",
    "        x1_normalized = tf.keras.backend.l2_normalize(x1, axis=1)\n",
    "        x2_normalized = tf.keras.backend.l2_normalize(x2, axis=1)\n",
    "        return x1_normalized * x2_normalized\n",
    "\n",
    "    output = tf.keras.layers.Lambda(normalized_product)([projected_1, projected_2])\n",
    "    output = tf.keras.layers.Dropout(0.1)(output)\n",
    "    output = tf.keras.layers.Dense(\n",
    "        16,\n",
    "        activation=\"relu\",\n",
    "    )(output)\n",
    "    output = tf.keras.layers.Dropout(0.2)(output)\n",
    "    output = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        activation=\"sigmoid\",\n",
    "    )(output)\n",
    "    \n",
    "    #output = tf.keras.layers.Lambda(lambda x: x * 5)(output)\n",
    "    #output = tf.keras.layers.Lambda(lambda x: scipy.special.logit(x))(output)\n",
    "    # Define output\n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training constants\n",
    "batch_size: int = 64\n",
    "num_epochs: int = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  \n",
    "    patience=5,          \n",
    "    restore_best_weights=True  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrada vàlida per al model\n",
    "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
    "\n",
    "    _x, _y = zip(*pair_list)\n",
    "    _x_1, _x_2 = zip(*_x)\n",
    "    return (np.array(_x_1), np.array(_x_2)), np.array(_y, dtype=np.float32, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Model evaluation**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns = [\"Pearson_train\",\"Spearman_train\",\"Pearson_val\",\"Spearman_val\"],index = [\"BOW\",\"TF-IDF\",\"W2VEC+TF-IDF\",\"W2VEC+MEAN\",\"SPACY_MD\",\"RoBERTa + CLS\",\"RoBERTa + MEAN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson_spearman(x_, y_,model):\n",
    "    # Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\n",
    "    y_pred = model.predict(x_)\n",
    "    print(np.max(y_pred))\n",
    "    # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "    pearson, _ = pearsonr(y_pred.flatten(), y_.flatten())\n",
    "    spearman,_ = spearmanr(y_pred.flatten(), y_.flatten())\n",
    "    return pearson, spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**BOW**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separació x-y\n",
    "x_train_bow, y_train_bow = pair_list_to_x_y(df_bow_train)\n",
    "x_val_bow, y_val_bow = pair_list_to_x_y(df_bow_val)\n",
    "\n",
    "#Preparar el conjunt\n",
    "\n",
    "train_bow = tf.data.Dataset.from_tensor_slices((x_train_bow, y_train_bow))\n",
    "train_bow = train_bow.shuffle(buffer_size=len(x_train_bow)).batch(batch_size)\n",
    "\n",
    "val__bow = tf.data.Dataset.from_tensor_slices((x_val_bow, y_val_bow))\n",
    "val__bow = val__bow.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir y compilar el modelo\n",
    "model_bow = build_and_compile_model_better()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4.9172 - val_loss: 4.8921\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7522 - val_loss: 4.6805\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5239 - val_loss: 4.4108\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2576 - val_loss: 4.1278\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9918 - val_loss: 3.8799\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7730 - val_loss: 3.6918\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6008 - val_loss: 3.5590\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4831 - val_loss: 3.4674\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3945 - val_loss: 3.4043\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3478 - val_loss: 3.3594\n",
      "Epoch 11/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3000 - val_loss: 3.3274\n",
      "Epoch 12/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2727 - val_loss: 3.3038\n",
      "Epoch 13/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2561 - val_loss: 3.2857\n",
      "Epoch 14/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2343 - val_loss: 3.2718\n",
      "Epoch 15/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2173 - val_loss: 3.2611\n",
      "Epoch 16/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2094 - val_loss: 3.2524\n",
      "Epoch 17/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1987 - val_loss: 3.2456\n",
      "Epoch 18/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1944 - val_loss: 3.2400\n",
      "Epoch 19/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1817 - val_loss: 3.2356\n",
      "Epoch 20/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1810 - val_loss: 3.2318\n",
      "Epoch 21/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1751 - val_loss: 3.2286\n",
      "Epoch 22/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1724 - val_loss: 3.2260\n",
      "Epoch 23/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1675 - val_loss: 3.2238\n",
      "Epoch 24/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1657 - val_loss: 3.2219\n",
      "Epoch 25/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1617 - val_loss: 3.2203\n",
      "Epoch 26/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1617 - val_loss: 3.2189\n",
      "Epoch 27/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1586 - val_loss: 3.2177\n",
      "Epoch 28/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1598 - val_loss: 3.2166\n",
      "Epoch 29/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1582 - val_loss: 3.2157\n",
      "Epoch 30/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1553 - val_loss: 3.2150\n",
      "Epoch 31/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1548 - val_loss: 3.2143\n",
      "Epoch 32/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1506 - val_loss: 3.2138\n",
      "Epoch 33/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1488 - val_loss: 3.2133\n",
      "Epoch 34/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1498 - val_loss: 3.2128\n",
      "Epoch 35/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1525 - val_loss: 3.2124\n",
      "Epoch 36/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1504 - val_loss: 3.2121\n",
      "Epoch 37/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1515 - val_loss: 3.2118\n",
      "Epoch 38/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1476 - val_loss: 3.2115\n",
      "Epoch 39/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1464 - val_loss: 3.2113\n",
      "Epoch 40/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1494 - val_loss: 3.2111\n",
      "Epoch 41/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1467 - val_loss: 3.2109\n",
      "Epoch 42/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1479 - val_loss: 3.2108\n",
      "Epoch 43/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1466 - val_loss: 3.2106\n",
      "Epoch 44/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1458 - val_loss: 3.2105\n",
      "Epoch 45/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1448 - val_loss: 3.2104\n",
      "Epoch 46/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1460 - val_loss: 3.2103\n",
      "Epoch 47/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1461 - val_loss: 3.2101\n",
      "Epoch 48/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1433 - val_loss: 3.2100\n",
      "Epoch 49/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1459 - val_loss: 3.2100\n",
      "Epoch 50/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1435 - val_loss: 3.2099\n",
      "Epoch 51/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1473 - val_loss: 3.2098\n",
      "Epoch 52/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1456 - val_loss: 3.2097\n",
      "Epoch 53/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1454 - val_loss: 3.2097\n",
      "Epoch 54/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1449 - val_loss: 3.2096\n",
      "Epoch 55/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1451 - val_loss: 3.2096\n",
      "Epoch 56/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1439 - val_loss: 3.2095\n",
      "Epoch 57/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1421 - val_loss: 3.2095\n",
      "Epoch 58/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1431 - val_loss: 3.2094\n",
      "Epoch 59/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1430 - val_loss: 3.2094\n",
      "Epoch 60/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1438 - val_loss: 3.2094\n",
      "Epoch 61/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1427 - val_loss: 3.2094\n",
      "Epoch 62/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1427 - val_loss: 3.2093\n",
      "Epoch 63/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1406 - val_loss: 3.2093\n",
      "Epoch 64/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1435 - val_loss: 3.2093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b6cf496750>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model_bow.fit(train_bow, epochs=num_epochs, validation_data=val__bow, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "0.999939\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "0.999939\n"
     ]
    }
   ],
   "source": [
    "pearson_train_bow, spearman_train_bow = compute_pearson_spearman(x_train_bow, y_train_bow,model_bow)\n",
    "pearson_val_bow, spearman_val_bow = compute_pearson_spearman(x_val_bow, y_val_bow,model_bow)\n",
    "\n",
    "results_df.loc[\"BOW\"] = [pearson_train_bow,spearman_train_bow,pearson_val_bow,spearman_val_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson_train</th>\n",
       "      <th>Spearman_train</th>\n",
       "      <th>Pearson_val</th>\n",
       "      <th>Spearman_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BOW</th>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.066097</td>\n",
       "      <td>0.08851</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+TF-IDF</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+MEAN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPACY_MD</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + CLS</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + MEAN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pearson_train Spearman_train Pearson_val Spearman_val\n",
       "BOW                 0.074408       0.066097     0.08851       0.0912\n",
       "TF-IDF                   NaN            NaN         NaN          NaN\n",
       "W2VEC+TF-IDF             NaN            NaN         NaN          NaN\n",
       "W2VEC+MEAN               NaN            NaN         NaN          NaN\n",
       "SPACY_MD                 NaN            NaN         NaN          NaN\n",
       "RoBERTa + CLS            NaN            NaN         NaN          NaN\n",
       "RoBERTa + MEAN           NaN            NaN         NaN          NaN"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**TF-IDF**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separació x-y\n",
    "x_train_tfidf, y_train_tfidf = pair_list_to_x_y(df_tfidf_train)\n",
    "x_val_tfidf, y_val_tfidf = pair_list_to_x_y(df_tfidf_val)\n",
    "\n",
    "#Preparar el conjunt\n",
    "\n",
    "train_tfidf = tf.data.Dataset.from_tensor_slices((x_train_tfidf, y_train_tfidf))\n",
    "train_tfidf = train_tfidf.shuffle(buffer_size=len(x_train_tfidf)).batch(batch_size)\n",
    "\n",
    "val_tfidf = tf.data.Dataset.from_tensor_slices((x_val_tfidf, y_val_tfidf))\n",
    "val_tfidf = val_tfidf.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir y compilar el modelo\n",
    "model_tfidf = build_and_compile_model_better()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4.9041 - val_loss: 4.8626\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7105 - val_loss: 4.5944\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4210 - val_loss: 4.2664\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1074 - val_loss: 3.9596\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8287 - val_loss: 3.7232\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6079 - val_loss: 3.5618\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4748 - val_loss: 3.4560\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3857 - val_loss: 3.3870\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3164 - val_loss: 3.3414\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2735 - val_loss: 3.3101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b6cfe13090>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tfidf.fit(train_tfidf, epochs=num_epochs, validation_data=val_tfidf, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "0.5901611\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "0.5803148\n"
     ]
    }
   ],
   "source": [
    "# Avaluació\n",
    "\n",
    "pearson_train_tfidf, spearman_train_tfidf = compute_pearson_spearman(x_train_tfidf, y_train_tfidf,model_tfidf)\n",
    "pearson_val_tfidf, spearman_val_tfidf = compute_pearson_spearman(x_val_tfidf, y_val_tfidf,model_tfidf)\n",
    "\n",
    "results_df.loc[\"TF-IDF\"] = [pearson_train_tfidf,spearman_train_tfidf,pearson_val_tfidf,spearman_val_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson_train</th>\n",
       "      <th>Spearman_train</th>\n",
       "      <th>Pearson_val</th>\n",
       "      <th>Spearman_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BOW</th>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.066097</td>\n",
       "      <td>0.08851</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.07461</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>0.065066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+TF-IDF</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+MEAN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPACY_MD</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + CLS</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + MEAN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pearson_train Spearman_train Pearson_val Spearman_val\n",
       "BOW                 0.074408       0.066097     0.08851       0.0912\n",
       "TF-IDF               0.07461       0.076763    0.079124     0.065066\n",
       "W2VEC+TF-IDF             NaN            NaN         NaN          NaN\n",
       "W2VEC+MEAN               NaN            NaN         NaN          NaN\n",
       "SPACY_MD                 NaN            NaN         NaN          NaN\n",
       "RoBERTa + CLS            NaN            NaN         NaN          NaN\n",
       "RoBERTa + MEAN           NaN            NaN         NaN          NaN"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**W2VEC + TF-IDF**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separació x-y\n",
    "x_train_w2vec_tf, y_train_w2vec_tf = pair_list_to_x_y(df_w2vec_tf_train)\n",
    "x_val_w2vec_tf, y_val_w2vec_tf = pair_list_to_x_y(df_w2vec_tf_val)\n",
    "\n",
    "#Preparar el conjunt\n",
    "\n",
    "train_w2vec_tf = tf.data.Dataset.from_tensor_slices((x_train_w2vec_tf, y_train_w2vec_tf))\n",
    "train_w2vec_tf = train_w2vec_tf.shuffle(buffer_size=len(x_train_w2vec_tf)).batch(batch_size)\n",
    "\n",
    "val_w2vec_tf = tf.data.Dataset.from_tensor_slices((x_val_w2vec_tf, y_val_w2vec_tf))\n",
    "val_w2vec_tf = val_w2vec_tf.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir y compilar el modelo\n",
    "model_w2vec_tf = build_and_compile_model_better(embedding_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4.8710 - val_loss: 4.7330\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5763 - val_loss: 4.4575\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3022 - val_loss: 4.1825\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0478 - val_loss: 3.9335\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8205 - val_loss: 3.7328\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6416 - val_loss: 3.5841\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5085 - val_loss: 3.4792\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4191 - val_loss: 3.4082\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3585 - val_loss: 3.3595\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3211 - val_loss: 3.3259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b6ff3100d0>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2vec_tf.fit(train_w2vec_tf, epochs=num_epochs, validation_data=val_w2vec_tf, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "0.58234954\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "0.58118546\n"
     ]
    }
   ],
   "source": [
    "# Avaluació\n",
    "\n",
    "pearson_train_w2vec_tf, spearman_train_w2vec_tf = compute_pearson_spearman(x_train_w2vec_tf, y_train_w2vec_tf,model_w2vec_tf)\n",
    "pearson_val_w2vec_tf, spearman_val_w2vec_tf = compute_pearson_spearman(x_val_w2vec_tf, y_val_w2vec_tf,model_w2vec_tf)\n",
    "\n",
    "results_df.loc[\"W2VEC+TF-IDF\"] = [pearson_train_w2vec_tf,spearman_train_w2vec_tf,pearson_val_w2vec_tf,spearman_val_w2vec_tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson_train</th>\n",
       "      <th>Spearman_train</th>\n",
       "      <th>Pearson_val</th>\n",
       "      <th>Spearman_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BOW</th>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.066097</td>\n",
       "      <td>0.08851</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.07461</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>0.065066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+TF-IDF</th>\n",
       "      <td>0.11843</td>\n",
       "      <td>0.13163</td>\n",
       "      <td>0.110207</td>\n",
       "      <td>0.130644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+MEAN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPACY_MD</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + CLS</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + MEAN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pearson_train Spearman_train Pearson_val Spearman_val\n",
       "BOW                 0.074408       0.066097     0.08851       0.0912\n",
       "TF-IDF               0.07461       0.076763    0.079124     0.065066\n",
       "W2VEC+TF-IDF         0.11843        0.13163    0.110207     0.130644\n",
       "W2VEC+MEAN               NaN            NaN         NaN          NaN\n",
       "SPACY_MD                 NaN            NaN         NaN          NaN\n",
       "RoBERTa + CLS            NaN            NaN         NaN          NaN\n",
       "RoBERTa + MEAN           NaN            NaN         NaN          NaN"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**W2VEC + MEAN**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separació x-y\n",
    "x_train_w2vec, y_train_w2vec = pair_list_to_x_y(df_w2vec_train)\n",
    "x_val_w2vec, y_val_w2vec = pair_list_to_x_y(df_w2vec_val)\n",
    "\n",
    "#Preparar el conjunt\n",
    "\n",
    "train_w2vec = tf.data.Dataset.from_tensor_slices((x_train_w2vec, y_train_w2vec))\n",
    "train_w2vec = train_w2vec.shuffle(buffer_size=len(x_train_w2vec)).batch(batch_size)\n",
    "\n",
    "val_w2vec = tf.data.Dataset.from_tensor_slices((x_val_w2vec, y_val_w2vec))\n",
    "val_w2vec = val_w2vec.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir y compilar el modelo\n",
    "model_w2vec = build_and_compile_model_better(embedding_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.8108 - val_loss: 4.5522\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3567 - val_loss: 4.1369\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9727 - val_loss: 3.8050\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6853 - val_loss: 3.5823\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4881 - val_loss: 3.4480\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3693 - val_loss: 3.3682\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3040 - val_loss: 3.3187\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2603 - val_loss: 3.2876\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2281 - val_loss: 3.2673\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2168 - val_loss: 3.2536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b6fdce5f10>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2vec.fit(train_w2vec, epochs=num_epochs, validation_data=val_w2vec, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "0.630656\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "0.6303681\n"
     ]
    }
   ],
   "source": [
    "# Avaluació\n",
    "\n",
    "pearson_train_w2vec, spearman_train_w2vec = compute_pearson_spearman(x_train_w2vec, y_train_w2vec,model_w2vec)\n",
    "pearson_val_w2vec, spearman_val_w2vec = compute_pearson_spearman(x_val_w2vec, y_val_w2vec,model_w2vec)\n",
    "\n",
    "results_df.loc[\"W2VEC+MEAN\"] = [pearson_train_w2vec,spearman_train_w2vec,pearson_val_w2vec,spearman_val_w2vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson_train</th>\n",
       "      <th>Spearman_train</th>\n",
       "      <th>Pearson_val</th>\n",
       "      <th>Spearman_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BOW</th>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.066097</td>\n",
       "      <td>0.08851</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.07461</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>0.065066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+TF-IDF</th>\n",
       "      <td>0.11843</td>\n",
       "      <td>0.13163</td>\n",
       "      <td>0.110207</td>\n",
       "      <td>0.130644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+MEAN</th>\n",
       "      <td>0.085555</td>\n",
       "      <td>0.100912</td>\n",
       "      <td>0.091003</td>\n",
       "      <td>0.109372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPACY_MD</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + CLS</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + MEAN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pearson_train Spearman_train Pearson_val Spearman_val\n",
       "BOW                 0.074408       0.066097     0.08851       0.0912\n",
       "TF-IDF               0.07461       0.076763    0.079124     0.065066\n",
       "W2VEC+TF-IDF         0.11843        0.13163    0.110207     0.130644\n",
       "W2VEC+MEAN          0.085555       0.100912    0.091003     0.109372\n",
       "SPACY_MD                 NaN            NaN         NaN          NaN\n",
       "RoBERTa + CLS            NaN            NaN         NaN          NaN\n",
       "RoBERTa + MEAN           NaN            NaN         NaN          NaN"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**SPACY_MD**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separació x-y\n",
    "x_train_spacy, y_train_spacy = pair_list_to_x_y(df_spacy_train)\n",
    "x_val_spacy, y_val_spacy = pair_list_to_x_y(df_spacy_val)\n",
    "\n",
    "#Preparar el conjunt\n",
    "\n",
    "train_spacy = tf.data.Dataset.from_tensor_slices((x_train_spacy, y_train_spacy))\n",
    "train_spacy = train_spacy.shuffle(buffer_size=len(x_train_spacy)).batch(batch_size)\n",
    "\n",
    "val_spacy = tf.data.Dataset.from_tensor_slices((x_val_spacy, y_val_spacy))\n",
    "val_spacy = val_spacy.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir y compilar el modelo\n",
    "model_spacy = build_and_compile_model_better()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4.8827 - val_loss: 4.8177\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6706 - val_loss: 4.5747\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4156 - val_loss: 4.2830\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1412 - val_loss: 4.0016\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8695 - val_loss: 3.7696\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6743 - val_loss: 3.5961\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5117 - val_loss: 3.4703\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3994 - val_loss: 3.3887\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3243 - val_loss: 3.3368\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2860 - val_loss: 3.3029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b68290e2d0>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spacy.fit(train_spacy, epochs=num_epochs, validation_data=val_spacy, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "0.5545029\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "0.5543672\n"
     ]
    }
   ],
   "source": [
    "# Avaluació\n",
    "\n",
    "pearson_train_spacy, spearman_train_spacy = compute_pearson_spearman(x_train_spacy, y_train_spacy,model_spacy)\n",
    "pearson_val_spacy, spearman_val_spacy = compute_pearson_spearman(x_val_spacy, y_val_spacy,model_spacy)\n",
    "\n",
    "results_df.loc[\"SPACY_MD\"] = [pearson_train_spacy,spearman_train_spacy,pearson_val_spacy,spearman_val_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson_train</th>\n",
       "      <th>Spearman_train</th>\n",
       "      <th>Pearson_val</th>\n",
       "      <th>Spearman_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BOW</th>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.066097</td>\n",
       "      <td>0.08851</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.07461</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>0.065066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+TF-IDF</th>\n",
       "      <td>0.11843</td>\n",
       "      <td>0.13163</td>\n",
       "      <td>0.110207</td>\n",
       "      <td>0.130644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+MEAN</th>\n",
       "      <td>0.085555</td>\n",
       "      <td>0.100912</td>\n",
       "      <td>0.091003</td>\n",
       "      <td>0.109372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPACY_MD</th>\n",
       "      <td>0.0739</td>\n",
       "      <td>0.140159</td>\n",
       "      <td>0.05252</td>\n",
       "      <td>0.116031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + CLS</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + MEAN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pearson_train Spearman_train Pearson_val Spearman_val\n",
       "BOW                 0.074408       0.066097     0.08851       0.0912\n",
       "TF-IDF               0.07461       0.076763    0.079124     0.065066\n",
       "W2VEC+TF-IDF         0.11843        0.13163    0.110207     0.130644\n",
       "W2VEC+MEAN          0.085555       0.100912    0.091003     0.109372\n",
       "SPACY_MD              0.0739       0.140159     0.05252     0.116031\n",
       "RoBERTa + CLS            NaN            NaN         NaN          NaN\n",
       "RoBERTa + MEAN           NaN            NaN         NaN          NaN"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**RoBERTa + CLS**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separació x-y\n",
    "x_train_bert_cls, y_train_bert_cls = pair_list_to_x_y(df_BERT_CLS)\n",
    "x_val_bert_cls, y_val_bert_cls = pair_list_to_x_y(df_BERT_CLS_val)\n",
    "\n",
    "#Preparar el conjunt\n",
    "\n",
    "train_bert_cls = tf.data.Dataset.from_tensor_slices((x_train_bert_cls, y_train_bert_cls))\n",
    "train_bert_cls = train_bert_cls.shuffle(buffer_size=len(x_train_bert_cls)).batch(batch_size)\n",
    "\n",
    "val_bert_cls = tf.data.Dataset.from_tensor_slices((x_val_bert_cls, y_val_bert_cls))\n",
    "val_bert_cls = val_bert_cls.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir y compilar el modelo\n",
    "model_bert_cls = build_and_compile_model_better(embedding_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.8614 - val_loss: 4.7512\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5929 - val_loss: 4.4473\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2833 - val_loss: 4.1090\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9575 - val_loss: 3.8144\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6909 - val_loss: 3.6070\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5225 - val_loss: 3.4772\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4115 - val_loss: 3.3959\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3374 - val_loss: 3.3401\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2809 - val_loss: 3.3023\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2487 - val_loss: 3.2775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b67581c3d0>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model_bert_cls.fit(train_bert_cls, epochs=num_epochs, validation_data=val_bert_cls, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "0.5713446\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "0.5713316\n"
     ]
    }
   ],
   "source": [
    "# Avaluació\n",
    "\n",
    "pearson_train_bert_cls, spearman_train_bert_cls = compute_pearson_spearman(x_train_bert_cls, y_train_bert_cls,model_bert_cls)\n",
    "pearson_val_bert_cls, spearman_val_bert_cls = compute_pearson_spearman(x_val_bert_cls, y_val_bert_cls,model_bert_cls)\n",
    "\n",
    "results_df.loc[\"RoBERTa + CLS\"] = [pearson_train_bert_cls,spearman_train_bert_cls,pearson_val_bert_cls,spearman_val_bert_cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**RoBERTa + MEAN**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separació x-y\n",
    "x_train_bert_mean, y_train_bert_mean = pair_list_to_x_y(df_BERT_MEAN)\n",
    "x_val_bert_mean, y_val_bert_mean = pair_list_to_x_y(df_BERT_MEAN_val)\n",
    "\n",
    "#Preparar el conjunt\n",
    "\n",
    "train_bert_mean = tf.data.Dataset.from_tensor_slices((x_train_bert_mean, y_train_bert_mean))\n",
    "train_bert_mean = train_bert_mean.shuffle(buffer_size=len(x_train_bert_mean)).batch(batch_size)\n",
    "\n",
    "val_bert_mean = tf.data.Dataset.from_tensor_slices((x_val_bert_mean, y_val_bert_mean))\n",
    "val_bert_mean = val_bert_mean.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir y compilar el modelo\n",
    "model_bert_mean = build_and_compile_model_better(embedding_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 4.8515 - val_loss: 4.7356\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5766 - val_loss: 4.4392\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2817 - val_loss: 4.1299\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9863 - val_loss: 3.8493\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7330 - val_loss: 3.6351\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5503 - val_loss: 3.4974\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4401 - val_loss: 3.4147\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3574 - val_loss: 3.3623\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3186 - val_loss: 3.3262\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2902 - val_loss: 3.3011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b6d55ccf90>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model_bert_mean.fit(train_bert_mean, epochs=num_epochs, validation_data=val_bert_mean, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "0.57380164\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "0.5737317\n"
     ]
    }
   ],
   "source": [
    "# Avaluació\n",
    "\n",
    "pearson_train_bert_mean, spearman_train_bert_mean = compute_pearson_spearman(x_train_bert_mean, y_train_bert_mean,model_bert_mean)\n",
    "pearson_val_bert_mean, spearman_val_bert_mean = compute_pearson_spearman(x_val_bert_mean, y_val_bert_mean,model_bert_mean)\n",
    "\n",
    "results_df.loc[\"RoBERTa + MEAN\"] = [pearson_train_bert_mean,spearman_train_bert_mean,pearson_val_bert_mean,spearman_val_bert_mean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson_train</th>\n",
       "      <th>Spearman_train</th>\n",
       "      <th>Pearson_val</th>\n",
       "      <th>Spearman_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BOW</th>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.066097</td>\n",
       "      <td>0.08851</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.07461</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>0.065066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+TF-IDF</th>\n",
       "      <td>0.11843</td>\n",
       "      <td>0.13163</td>\n",
       "      <td>0.110207</td>\n",
       "      <td>0.130644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+MEAN</th>\n",
       "      <td>0.085555</td>\n",
       "      <td>0.100912</td>\n",
       "      <td>0.091003</td>\n",
       "      <td>0.109372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPACY_MD</th>\n",
       "      <td>0.0739</td>\n",
       "      <td>0.140159</td>\n",
       "      <td>0.05252</td>\n",
       "      <td>0.116031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + CLS</th>\n",
       "      <td>0.092978</td>\n",
       "      <td>0.108363</td>\n",
       "      <td>-0.003979</td>\n",
       "      <td>0.031287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + MEAN</th>\n",
       "      <td>0.166425</td>\n",
       "      <td>0.160903</td>\n",
       "      <td>0.106723</td>\n",
       "      <td>0.057881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pearson_train Spearman_train Pearson_val Spearman_val\n",
       "BOW                 0.074408       0.066097     0.08851       0.0912\n",
       "TF-IDF               0.07461       0.076763    0.079124     0.065066\n",
       "W2VEC+TF-IDF         0.11843        0.13163    0.110207     0.130644\n",
       "W2VEC+MEAN          0.085555       0.100912    0.091003     0.109372\n",
       "SPACY_MD              0.0739       0.140159     0.05252     0.116031\n",
       "RoBERTa + CLS       0.092978       0.108363   -0.003979     0.031287\n",
       "RoBERTa + MEAN      0.166425       0.160903    0.106723     0.057881"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**RoBERTa finetuned**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get x and y from pairs\n",
    "def get_x_y(pairs):\n",
    "    x = [(a[0],a[1]) for a in pairs]\n",
    "    y = [a[2] for a in pairs]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jordi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and preprocessing\n",
    "model = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipe = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def prepare(sentence_pairs):\n",
    "    sentence_pairs_prep = []\n",
    "    for s1, s2 in sentence_pairs:\n",
    "        sentence_pairs_prep.append(f\"{tokenizer.cls_token} {s1}{tokenizer.sep_token}{tokenizer.sep_token} {s2}{tokenizer.sep_token}\")\n",
    "    return sentence_pairs_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_finetuned,y_train_finetuned = get_x_y(input_pairs)\n",
    "x_val_finetuned,y_val_finetuned = get_x_y(input_pairs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "predictions_train = pipe(prepare(x_train_finetuned), add_special_tokens=False)\n",
    "predictions_val = pipe(prepare(x_val_finetuned), add_special_tokens=False)\n",
    "\n",
    "# convert back to scores to the original 0 and 5 interval\n",
    "for prediction in predictions_train:\n",
    "    prediction['score'] = scipy.special.logit(prediction['score'])\n",
    "for prediction in predictions_val:\n",
    "    prediction['score'] = scipy.special.logit(prediction['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_spearman(y_true, y_pred):\n",
    "    # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "    pearson, _ = pearsonr(y_pred.flatten(), y_true.flatten())\n",
    "    spearman,_ = spearmanr(y_pred.flatten(), y_true.flatten())\n",
    "    return pearson, spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Pearson and Spearman correlation\n",
    "\n",
    "pearson_train_finetuned, spearman_train_finetuned = pearson_spearman(np.array(y_train_finetuned), np.array([p['score'] for p in predictions_train]))\n",
    "pearson_val_finetuned, spearman_val_finetuned = pearson_spearman(np.array(y_val_finetuned), np.array([p['score'] for p in predictions_val]))\n",
    "\n",
    "results_df.loc[\"RoBERTa + Finetuned\"] = [pearson_train_finetuned,spearman_train_finetuned,pearson_val_finetuned,spearman_val_finetuned]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson_train</th>\n",
       "      <th>Spearman_train</th>\n",
       "      <th>Pearson_val</th>\n",
       "      <th>Spearman_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BOW</th>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.066097</td>\n",
       "      <td>0.08851</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.07461</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>0.065066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+TF-IDF</th>\n",
       "      <td>0.11843</td>\n",
       "      <td>0.13163</td>\n",
       "      <td>0.110207</td>\n",
       "      <td>0.130644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+MEAN</th>\n",
       "      <td>0.085555</td>\n",
       "      <td>0.100912</td>\n",
       "      <td>0.091003</td>\n",
       "      <td>0.109372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPACY_MD</th>\n",
       "      <td>0.0739</td>\n",
       "      <td>0.140159</td>\n",
       "      <td>0.05252</td>\n",
       "      <td>0.116031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + CLS</th>\n",
       "      <td>0.092978</td>\n",
       "      <td>0.108363</td>\n",
       "      <td>-0.003979</td>\n",
       "      <td>0.031287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + MEAN</th>\n",
       "      <td>0.166425</td>\n",
       "      <td>0.160903</td>\n",
       "      <td>0.106723</td>\n",
       "      <td>0.057881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + Finetuned</th>\n",
       "      <td>0.947429</td>\n",
       "      <td>0.961433</td>\n",
       "      <td>0.75226</td>\n",
       "      <td>0.731941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Pearson_train Spearman_train Pearson_val Spearman_val\n",
       "BOW                      0.074408       0.066097     0.08851       0.0912\n",
       "TF-IDF                    0.07461       0.076763    0.079124     0.065066\n",
       "W2VEC+TF-IDF              0.11843        0.13163    0.110207     0.130644\n",
       "W2VEC+MEAN               0.085555       0.100912    0.091003     0.109372\n",
       "SPACY_MD                   0.0739       0.140159     0.05252     0.116031\n",
       "RoBERTa + CLS            0.092978       0.108363   -0.003979     0.031287\n",
       "RoBERTa + MEAN           0.166425       0.160903    0.106723     0.057881\n",
       "RoBERTa + Finetuned      0.947429       0.961433     0.75226     0.731941"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jordi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\spatial\\distance.py:647: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / math.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "# Make another table, but the predictions are made with cosine distance\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "results_cosine = pd.DataFrame(columns = [\"Pearson_train\",\"Spearman_train\",\"Pearson_val\",\"Spearman_val\"],index = [\"BOW\",\"TF-IDF\",\"W2VEC+TF-IDF\",\"W2VEC+MEAN\",\"SPACY_MD\",\"RoBERTa + CLS\",\"RoBERTa + MEAN\",\"RoBERTa + Finetuned\"]) \n",
    "\n",
    "# Baseline\n",
    "def compute_pearson_baseline(x_, y_):\n",
    "    y_pred_baseline = []\n",
    "    for v1, v2 in zip(*x_):\n",
    "        d = 1.0 - scipy.spatial.distance.cosine(v1, v2)\n",
    "        y_pred_baseline.append(d)\n",
    "    # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "    correlation, _ = pearsonr(y_pred_baseline, y_.flatten()*5)\n",
    "    return correlation\n",
    "\n",
    "pearson_train_bow_baseline = compute_pearson_baseline(x_train_bow, y_train_bow)\n",
    "pearson_val_bow_baseline = compute_pearson_baseline(x_val_bow, y_val_bow)\n",
    "\n",
    "results_cosine.loc[\"BOW\"] = [pearson_train_bow_baseline,0,pearson_val_bow_baseline,0]\n",
    "\n",
    "# TF-IDF\n",
    "pearson_train_tfidf_baseline = compute_pearson_baseline(x_train_tfidf, y_train_tfidf)\n",
    "pearson_val_tfidf_baseline = compute_pearson_baseline(x_val_tfidf, y_val_tfidf)\n",
    "\n",
    "results_cosine.loc[\"TF-IDF\"] = [pearson_train_tfidf_baseline,0,pearson_val_tfidf_baseline,0]\n",
    "\n",
    "# W2VEC + TF-IDF\n",
    "pearson_train_w2vec_tf_baseline = compute_pearson_baseline(x_train_w2vec_tf, y_train_w2vec_tf)\n",
    "pearson_val_w2vec_tf_baseline = compute_pearson_baseline(x_val_w2vec_tf, y_val_w2vec_tf)\n",
    "\n",
    "results_cosine.loc[\"W2VEC+TF-IDF\"] = [pearson_train_w2vec_tf_baseline,0,pearson_val_w2vec_tf_baseline,0]\n",
    "\n",
    "# W2VEC + MEAN\n",
    "pearson_train_w2vec_baseline = compute_pearson_baseline(x_train_w2vec, y_train_w2vec)\n",
    "pearson_val_w2vec_baseline = compute_pearson_baseline(x_val_w2vec, y_val_w2vec)\n",
    "\n",
    "results_cosine.loc[\"W2VEC+MEAN\"] = [pearson_train_w2vec_baseline,0,pearson_val_w2vec_baseline,0]\n",
    "\n",
    "# SPACY\n",
    "pearson_train_spacy_baseline = compute_pearson_baseline(x_train_spacy, y_train_spacy)\n",
    "pearson_val_spacy_baseline = compute_pearson_baseline(x_val_spacy, y_val_spacy)\n",
    "\n",
    "results_cosine.loc[\"SPACY_MD\"] = [pearson_train_spacy_baseline,0,pearson_val_spacy_baseline,0]\n",
    "\n",
    "# RoBERTa + CLS\n",
    "pearson_train_bert_cls_baseline = compute_pearson_baseline(x_train_bert_cls, y_train_bert_cls)\n",
    "pearson_val_bert_cls_baseline = compute_pearson_baseline(x_val_bert_cls, y_val_bert_cls)\n",
    "\n",
    "results_cosine.loc[\"RoBERTa + CLS\"] = [pearson_train_bert_cls_baseline,0,pearson_val_bert_cls_baseline,0]\n",
    "\n",
    "# RoBERTa + MEAN\n",
    "pearson_train_bert_mean_baseline = compute_pearson_baseline(x_train_bert_mean, y_train_bert_mean)\n",
    "pearson_val_bert_mean_baseline = compute_pearson_baseline(x_val_bert_mean, y_val_bert_mean)\n",
    "\n",
    "results_cosine.loc[\"RoBERTa + MEAN\"] = [pearson_train_bert_mean_baseline,0,pearson_val_bert_mean_baseline,0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson_train</th>\n",
       "      <th>Spearman_train</th>\n",
       "      <th>Pearson_val</th>\n",
       "      <th>Spearman_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W2VEC+TF-IDF</th>\n",
       "      <td>0.427605</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+MEAN</th>\n",
       "      <td>0.42764</td>\n",
       "      <td>0</td>\n",
       "      <td>0.426903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + MEAN</th>\n",
       "      <td>0.366229</td>\n",
       "      <td>0</td>\n",
       "      <td>0.304729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.220896</td>\n",
       "      <td>0</td>\n",
       "      <td>0.215571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW</th>\n",
       "      <td>0.22873</td>\n",
       "      <td>0</td>\n",
       "      <td>0.210217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPACY_MD</th>\n",
       "      <td>0.236751</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + CLS</th>\n",
       "      <td>0.144417</td>\n",
       "      <td>0</td>\n",
       "      <td>0.094534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + Finetuned</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Pearson_train Spearman_train Pearson_val Spearman_val\n",
       "W2VEC+TF-IDF             0.427605              0     0.45547            0\n",
       "W2VEC+MEAN                0.42764              0    0.426903            0\n",
       "RoBERTa + MEAN           0.366229              0    0.304729            0\n",
       "TF-IDF                   0.220896              0    0.215571            0\n",
       "BOW                       0.22873              0    0.210217            0\n",
       "SPACY_MD                 0.236751              0    0.194279            0\n",
       "RoBERTa + CLS            0.144417              0    0.094534            0\n",
       "RoBERTa + Finetuned           NaN            NaN         NaN          NaN"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_cosine.sort_values(by = \"Pearson_val\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson_train</th>\n",
       "      <th>Spearman_train</th>\n",
       "      <th>Pearson_val</th>\n",
       "      <th>Spearman_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W2VEC+TF-IDF</th>\n",
       "      <td>0.11843</td>\n",
       "      <td>0.13163</td>\n",
       "      <td>0.110207</td>\n",
       "      <td>0.130644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + MEAN</th>\n",
       "      <td>0.166425</td>\n",
       "      <td>0.160903</td>\n",
       "      <td>0.106723</td>\n",
       "      <td>0.057881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W2VEC+MEAN</th>\n",
       "      <td>0.085555</td>\n",
       "      <td>0.100912</td>\n",
       "      <td>0.091003</td>\n",
       "      <td>0.109372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW</th>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.066097</td>\n",
       "      <td>0.08851</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.07461</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>0.065066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPACY_MD</th>\n",
       "      <td>0.0739</td>\n",
       "      <td>0.140159</td>\n",
       "      <td>0.05252</td>\n",
       "      <td>0.116031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + CLS</th>\n",
       "      <td>0.092978</td>\n",
       "      <td>0.108363</td>\n",
       "      <td>-0.003979</td>\n",
       "      <td>0.031287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pearson_train Spearman_train Pearson_val Spearman_val\n",
       "W2VEC+TF-IDF         0.11843        0.13163    0.110207     0.130644\n",
       "RoBERTa + MEAN      0.166425       0.160903    0.106723     0.057881\n",
       "W2VEC+MEAN          0.085555       0.100912    0.091003     0.109372\n",
       "BOW                 0.074408       0.066097     0.08851       0.0912\n",
       "TF-IDF               0.07461       0.076763    0.079124     0.065066\n",
       "SPACY_MD              0.0739       0.140159     0.05252     0.116031\n",
       "RoBERTa + CLS       0.092978       0.108363   -0.003979     0.031287"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by = \"Pearson_val\",ascending = False)[1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
